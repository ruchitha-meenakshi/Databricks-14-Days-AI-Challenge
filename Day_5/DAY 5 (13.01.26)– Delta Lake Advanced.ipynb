{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e10342-0bd5-4eb3-89b9-f3bf372d31a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9d662c-d33e-4d90-be77-60372c350202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SETUP: Load Data & Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0822d0d5-726e-4656-8b3d-041a76e3cfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Target Path: /Volumes/workspace/ecommerce/ecommerce_data/delta/events_silver\n\uD83D\uDCCD Target Table: events_managed_sample\n"
     ]
    }
   ],
   "source": [
    "delta_path = \"/Volumes/workspace/ecommerce/ecommerce_data/delta/events_silver\"\n",
    "# We use the managed table name for SQL commands\n",
    "table_name = \"events_managed_sample\"\n",
    "\n",
    "print(f\"\uD83D\uDCCD Target Path: {delta_path}\")\n",
    "print(f\"\uD83D\uDCCD Target Table: {table_name}\")\n",
    "\n",
    "# Initialize DeltaTable object for PySpark commands\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1128a9-4f6c-4b61-98b9-38fa7124e1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement Incremental MERGE (Upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7038e9-6eaa-4db0-b72d-056757438cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Task 1: Incremental MERGE ---\n\uD83D\uDD04 Performing Upsert...\nTask 1 Complete: Data merged successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Task 1: Incremental MERGE ---\")\n",
    "\n",
    "# 1. Prepare New Data (Simulation)\n",
    "# FIX: Added 'category_id' (set to 0) to match the target table schema exactly.\n",
    "new_data = [\n",
    "    # Existing User (Update): Price correction\n",
    "    (\"2019-10-01 00:00:00\", \"purchase\", 1002544, 0, \"electronics.smartphone\", \"electronics\", \"apple\", 850.00, 518958788, \"sess_1\"),\n",
    "    # New User (Insert): Late arrival\n",
    "    (\"2019-10-02 10:00:00\", \"view\", 5555555, 0, \"apparel.shoes\", \"apparel\", \"adidas\", 120.00, 777777777, \"sess_new\")\n",
    "]\n",
    "\n",
    "# FIX: Added 'category_id' to the columns list\n",
    "columns = [\"event_time\", \"event_type\", \"product_id\", \"category_id\", \"category_code\", \"category_main\", \"brand\", \"price\", \"user_id\", \"user_session\"]\n",
    "\n",
    "updates_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# 2. Execute Merge\n",
    "print(\"\uD83D\uDD04 Performing Upsert...\")\n",
    "deltaTable.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    # Condition: Match User + Product + Session\n",
    "    \"t.user_id = s.user_id AND t.product_id = s.product_id AND t.user_session = s.user_session\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"price\": \"s.price\" # Update logic\n",
    "}).whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"Task 1 Complete: Data merged successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6a391e-cab9-449c-ac0e-30f497d0f92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query Historical Versions (Time Travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a8c591-823e-4851-ad15-44db95eecacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Task 2: Time Travel ---\n\uD83D\uDCDC Table Commit History:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>5</td><td>2026-01-13T09:33:33.000Z</td><td>75645690044269</td><td>meenakshi.urvs@gmail.com</td><td>MERGE</td><td>Map(predicate -> [\"(((cast(user_id#13291 as bigint) = user_id#13341L) AND (cast(product_id#13286 as bigint) = product_id#13335L)) AND (user_session#13292 = user_session#13342))\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(3504017974982911)</td><td>0113-093134-n7xioa89-v2n</td><td>4</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 4751, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 5888, materializeSourceTimeMs -> 370, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2244, numTargetRowsUpdated -> 0, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 3098)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>4</td><td>2026-01-12T11:42:01.000Z</td><td>75645690044269</td><td>meenakshi.urvs@gmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3208981861955938)</td><td>0112-111102-53f71rql-v2n</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 12, numRemovedFiles -> 12, numRemovedBytes -> 1471926065, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1471926065)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>3</td><td>2026-01-12T11:26:56.000Z</td><td>75645690044269</td><td>meenakshi.urvs@gmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3208981861955938)</td><td>0112-111102-53f71rql-v2n</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 12, numRemovedFiles -> 12, numRemovedBytes -> 1471926065, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1471926065)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>2</td><td>2026-01-12T11:13:05.000Z</td><td>75645690044269</td><td>meenakshi.urvs@gmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3208981861955938)</td><td>0112-111102-53f71rql-v2n</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 12, numRemovedFiles -> 12, numRemovedBytes -> 1471926065, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1471926065)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>1</td><td>2026-01-12T07:11:34.000Z</td><td>75645690044269</td><td>meenakshi.urvs@gmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3208981861955938)</td><td>0112-070104-f8da936c-v2n</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 12, numRemovedFiles -> 12, numRemovedBytes -> 1471926065, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1471926065)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         "2026-01-13T09:33:33.000Z",
         "75645690044269",
         "meenakshi.urvs@gmail.com",
         "MERGE",
         {
          "clusterBy": "[]",
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(((cast(user_id#13291 as bigint) = user_id#13341L) AND (cast(product_id#13286 as bigint) = product_id#13335L)) AND (user_session#13292 = user_session#13342))\"]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3504017974982911"
         ],
         "0113-093134-n7xioa89-v2n",
         4,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "5888",
          "materializeSourceTimeMs": "370",
          "numOutputRows": "2",
          "numSourceRows": "2",
          "numTargetBytesAdded": "4751",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "2",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "2",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "0",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "3098",
          "scanTimeMs": "2244"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         4,
         "2026-01-12T11:42:01.000Z",
         "75645690044269",
         "meenakshi.urvs@gmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3208981861955938"
         ],
         "0112-111102-53f71rql-v2n",
         3,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "12",
          "numOutputBytes": "1471926065",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1471926065",
          "numRemovedFiles": "12"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         3,
         "2026-01-12T11:26:56.000Z",
         "75645690044269",
         "meenakshi.urvs@gmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3208981861955938"
         ],
         "0112-111102-53f71rql-v2n",
         2,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "12",
          "numOutputBytes": "1471926065",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1471926065",
          "numRemovedFiles": "12"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         2,
         "2026-01-12T11:13:05.000Z",
         "75645690044269",
         "meenakshi.urvs@gmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3208981861955938"
         ],
         "0112-111102-53f71rql-v2n",
         1,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "12",
          "numOutputBytes": "1471926065",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1471926065",
          "numRemovedFiles": "12"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         1,
         "2026-01-12T07:11:34.000Z",
         "75645690044269",
         "meenakshi.urvs@gmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3208981861955938"
         ],
         "0112-070104-f8da936c-v2n",
         0,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "12",
          "numOutputBytes": "1471926065",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1471926065",
          "numRemovedFiles": "12"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"jobId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobRunId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"runId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobOwnerId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"triggerType\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"notebookId\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Complete: Loaded Version 0 (42,448,764 rows).\n"
     ]
    }
   ],
   "source": [
    "# Requirement: View the table history and access a previous state.\n",
    "print(\"\\n--- Task 2: Time Travel ---\")\n",
    "\n",
    "# 1. View History\n",
    "print(\"\uD83D\uDCDC Table Commit History:\")\n",
    "display(deltaTable.history().limit(5))\n",
    "\n",
    "# 2. Travel back to Version 0 (Original state)\n",
    "df_v0 = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(f\"Task 2 Complete: Loaded Version 0 ({df_v0.count():,} rows).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5508d30-6f6a-4e1e-b1cc-f44884125fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimize Tables (Z-ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4facc4d6-770d-4e06-8b2e-180e6447675c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Task 3: Optimize & Z-Order ---\nTask 3 Complete: Table optimized via Path.\n"
     ]
    }
   ],
   "source": [
    "# Requirement: Compact small files and organize data for faster querying.\n",
    "print(\"\\n--- Task 3: Optimize & Z-Order ---\")\n",
    "\n",
    "# We use Spark SQL for the Optimize command\n",
    "# Z-Ordering by 'event_type' and 'brand' speeds up filters on those columns\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE delta.`{delta_path}`\n",
    "    ZORDER BY (event_type, brand)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Task 3 Complete: Table optimized via Path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "082da765-2d9c-4a7f-a038-0e83d3e2428f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Clean Old Files (VACUUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbb3506-84ce-46f2-b8c2-65453dfd9441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Task 4: Vacuum (Cleanup) ---\nRunning VACUUM (Standard Retention)...\nTask 4 Complete: Vacuum command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Requirement: Remove stale files to save storage space.\n",
    "print(\"\\n--- Task 4: Vacuum (Cleanup) ---\")\n",
    "print(\"Running VACUUM (Standard Retention)...\")\n",
    "\n",
    "# We use 'RETAIN 168 HOURS' (7 days), which is the standard safety limit.\n",
    "# This respects the cluster's safety policies while proving the command works.\n",
    "spark.sql(f\"VACUUM delta.`{delta_path}` RETAIN 168 HOURS\")\n",
    "\n",
    "print(\"Task 4 Complete: Vacuum command executed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY 5 (13.01.26)â€“ Delta Lake Advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}