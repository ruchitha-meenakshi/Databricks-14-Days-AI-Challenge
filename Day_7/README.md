# Day 7: Workflows & Job Orchestration ‚öôÔ∏è


**Challenge:** [Databricks 14-Day AI Challenge](https://www.linkedin.com/feed/hashtag/?keywords=databrickswithidc)  
**Topic:** Moving from Manual Notebooks to Automated Production Jobs  
<img width="700" height="1350" alt="7 (1)" src="https://github.com/user-attachments/assets/57f0dcdd-7f6e-4319-a42a-8246eda67e39" />  

![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Codebasics](https://img.shields.io/badge/Codebasics-F7931E?style=for-the-badge)
![Indian Data Club](https://img.shields.io/badge/Indian_Data_Club-4CAF50?style=for-the-badge)  
![Status](https://img.shields.io/badge/Status-Completed-green?style=for-the-badge)
---

## Deep Dive: Production Orchestration

Data Engineering isn't just about writing code; it's about making that code run reliably, automatically, and efficiently. Day 7 focuses on **Job Orchestration** using Databricks Workflows.

### 1. The "Generic Notebook" Pattern

I learned something interesting today, In the early days of Spark, engineers would write three separate notebooks: `01_ingest.py`, `02_clean.py`, `03_aggregate.py`. This is hard to maintain.

* **The Solution:** Parameterization (Widgets).
* **How it works:** I wrote a **single** notebook that acts as a generic logic engine. By passing a parameter (`layer='bronze'`), the notebook dynamically decides which function to execute.
* **Benefit:** DRY (Don't Repeat Yourself). If I need to change how I initialize the Spark session or handle errors, I change it in one place, not three.

### 2. Workflows & DAGs (Directed Acyclic Graphs)

A "Job" is a collection of tasks. To ensure data integrity, these tasks must run in a specific order.

* **Dependency Management:**
* We cannot run **Silver** (Cleaning) until **Bronze** (Ingestion) is 100% successful.
* We cannot run **Gold** (Aggregates) until **Silver** is clean.

* **The DAG:** This strict ordering creates a graph. If "Bronze" fails (e.g., due to a missing source file), the Workflow automatically stops. This prevents "data corruption" where we might accidentally aggregate old/stale data.

### 3. Interactive vs. Job Clusters

* **Interactive Clusters:** (What we used Day 1-6). Designed for humans. They stay on, waiting for us to type code. Expensive because they idle.
* **Job Clusters:** (What Workflows use). Designed for robots. They spin up when the job starts, run the code at maximum efficiency, and terminate immediately when finished. This is significantly **cheaper** and **cleaner** for production.

### 4. Robustness & Retries

In production, things break. APIs timeout, networks blip.

* **Automatic Retries:** I configured the job to automatically retry a failed task (e.g., up to 3 times).
* **Alerting:** If the job fails after retries, Databricks automatically sends an email/Slack alert. This distinguishes a "Hobby Project" from a "Production Pipeline."
---
## üöÄ Key Code Snippet
The core of this workflow is the **Execution Controller**. Instead of maintaining three separate notebooks, I implemented a "Switch" logic that dynamically decides which function to run based on the input parameter.

```python
# Execution Controller: Switches logic based on the 'layer' widget
if current_layer == "bronze":
    run_bronze()
elif current_layer == "silver":
    run_silver()
elif current_layer == "gold":
    run_gold()
else:
    # Fails the job if an invalid parameter is passed
    raise ValueError(f"Unknown layer: {current_layer}")
```
Here is the Directed Acyclic Graph (DAG) generated by this code in Databricks Workflows. It confirms that the dependencies were respected (Silver waited for Bronze, Gold waited for Silver).
<img width="617" height="151" alt="Screenshot 2026-01-15 at 09 02 56" src="https://github.com/user-attachments/assets/eb6df759-8c5c-4e14-88b9-983e135de90b" />

---
## Tasks
- [x] **Refactored Code:** Added `dbutils.widgets` to make the pipeline modular.
- [x] **Created Workflow:** Built a 3-step job (Bronze ‚Üí Silver ‚Üí Gold) in the Databricks UI.
- [x] **Dependencies:** Configured the job so downstream tasks wait for upstream success.
- [x] **Execution:** Successfully ran the end-to-end pipeline via the Jobs scheduler.
---
### üìÇ Repository Structure

* **`DAY 7 ‚Äì Workflows & Job Orchestration.ipynb`**: A single, modular notebook containing the logic for Bronze, Silver, and Gold layers, controlled by `dbutils.widgets`.
